\section{C\lc{onclusion} \lc{and} F\lc{uture} W\lc{ork}}
\label{sec:conclusion}


More than a decade ago, the emergence of cloud computing led to the
realization that applications had to be written in a certain way to
take full advantage of elasticity of the cloud.  This led to the
concept of ``cloud-native applications'' whose scale-out capabilities
are well matched to the cloud, as well as tools and techniques to
easily create such applications.

The emergence of edge computing leads to another inflection point in
application design.  In particular, it leads to ``edge-native
applications'' that are deeply dependent on attributes such as low
latency or bandwidth scalability that can only be obtained at the edge.
However, as this paper has shown, edge-native applications have to be 
written in a way that is very different from cloud-native applications
if they are to be scalable.

The is the first work to show that cloud-native implementation
strategies that focus primarily on dynamic scale-out are unlikely to
be effective for scalability in edge computing.  Instead, edge-native
applications need to adapt their network and cloudlet resource demand
to system load.  As the total number of Tier-3 devices associated with
a cloudlet increases, the per-device network and cloudlet load has to
decrease.  This is a fundamental difference between cloud-native and
edge-native approaches to scalability. 

In this paper, we explore client workload reduction and server resource
allocation to manage application quality of service in the face of contention
for cloudlet resources. We demonstrate that our system is able to ensure that in
overloaded situations, a subset of users are still served with good quality of
service rather than equally sharing resources and missing latency
requirements for all.

This work serves as an initial step towards practical resource management for
edge-native applications. There are many potential directions to explore further
in this space. We have alluded to some of these earlier in the paper. One
example we briefly mentioned is dynamic partitioning of work between Tier-3 and
Tier-2 to further reduce offered load on cloudlets.  In addition, other resource
allocation policies, especially fairness-centered policies, such as max-min
fairness and static priority can be explored when optimizing overall system
performance. These fairness-focused policies could also be used to address
aggressive users, which are not considered in this paper.  While we have shown
offline profiling is effective for predicting demand and utility for WCA
applications, for a broader range of edge-native applications, with ever more
aggressive and variable offload management, online estimation may prove to be
necessary. Another area worth exploring is the particular set of control and
coordination mechanisms to allow cloudlets to manage client offered load
directly. Finally, the implementation to date only contains allocation of
resources but allows the cloudlet operating system to arbitrarily schedule
application processes.  Whether fine-grained control of application scheduling
on cloudlets can help scale services remains an open question.

