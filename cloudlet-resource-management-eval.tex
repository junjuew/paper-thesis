\section{Evaluation}

We use five WCA applications, including FACE, PING PONG, LEGO, POOL, and IKEA
for evaluation~\cite{chen2017empirical,chen2018application}. These
applications are selected based on their distinct requirements and
characteristics to represent the variety of WCA apps. IKEA and LEGO assist users
step by step to assemble an IKEA lamp or a LEGO model. While their 2.7-second
loose latency bound is less stringent than other apps, the significance of their
instructions is high, as a user could not proceed without the instruction. On
the other hand, users could still continue their tasks without the instructions
from FACE, POOL, and PING PONG assistants. For POOL and PING PONG, the speed of
an instruction is paramount to its usefulness. For example, any instruction that
comes 105ms after a user action for POOL is no longer of value, because it is
too late to guide the next action.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth, trim=0em 3em 0em 3em, clip]{FIGS/fig-sec6-reduction-legend.pdf}
  \begin{minipage}[b]{0.38\linewidth}
    \centering
    \includegraphics[height=2.5in, trim=0em 1em 0em 0em, clip]{FIGS/fig-sec6-reduction-Pingpong.pdf}\\
    {(a) PING PONG}
  \end{minipage}
  \begin{minipage}[b]{0.3\linewidth}
    \centering
    \includegraphics[height=2.5in, trim=0em 1em 0em 0em, clip]{FIGS/fig-sec6-reduction-Lego.pdf}\\
    {(b) LEGO}
  \end{minipage}
  \begin{minipage}[b]{0.3\linewidth}
    \centering
    \includegraphics[height=2.5in, trim=0em 1em 0em 0em, clip]{FIGS/fig-sec6-reduction-Pool.pdf}\\
    {(c) POOL}
  \end{minipage}
  \caption{Effects of Workload Reduction}
  \label{figs:workload-reduction}
\end{figure}


\subsection{Effectiveness of Workload Reduction}

We first evaluate the effectiveness of all of the workload reduction techniques
explored in Chapter~\ref{chapter: load}. For this set of experiments, we do not
use multiple concurrent applications. Adaptation-centric cloudlet resource
allocation is not enabled for a controlled setup. We use four Nexus 6 mobile
phones as clients. They offload computation to a cloudlet over Wi-Fi links. We
run PING PONG, LEGO, and POOL applications one at a time with 2, 4, 6, and 8
cores on the edge server. We constrain the number of cores available using Linux
cgroup. Figure~\ref{figs:workload-reduction} shows the total number of frames
processed with and without workload reduction. The yellow lines for Original
Gabriel do not have workload reduction while the blue lines for Scalable Gabriel
do. The solid lines represent the total number of frames offloaded. The dashed
lines represent the number of active frames, those frames that actually contain
user state information. Note that although the offered work is greatly reduced,
the processed frames for active phases of the application have not been
affected. Thus, we confirm that we can significantly reduce cloudlet load
without affecting the critical processing needed by these applications.

\begin{table}[]
  \centering
  \begin{tabular}{|c|c||c|c|c|c|c|}
    \hline
    Exp & \multicolumn{6}{|c|}{Number of Clients}                                    \\
    \cline{2-7}
    \#  & Total                                   & FACE & LEGO & POOL & PING & IKEA \\
        &                                         &      &      &      & PONG &      \\ \hline
    1   & 15                                      & 3    & 3    & 3    & 3    & 3    \\ \hline
    2   & 20                                      & 4    & 4    & 4    & 4    & 4    \\ \hline
    3   & 23                                      & 5    & 5    & 4    & 4    & 5    \\ \hline
    4   & 25                                      & 5    & 5    & 5    & 5    & 5    \\ \hline
    5   & 27                                      & 5    & 6    & 6    & 5    & 5    \\ \hline
    6   & 30                                      & 5    & 7    & 6    & 6    & 6    \\ \hline
    7   & 32                                      & 5    & 7    & 7    & 7    & 6    \\ \hline
    8   & 40                                      & 8    & 8    & 8    & 8    & 8    \\ \hline
  \end{tabular}
  \vspace{0.1in}
  \caption{Resource Allocation Experiment Setup}
  \label{tab:alloc-exps}
\end{table}

\subsection{Effectiveness of Resource Allocation}

We next evaluate our adaptation-centric resource allocation mechanism on a
server machine with 2 Intel{\textregistered} Xeon{\textregistered} E5-2699 v3
processors, totaling 36 physical cores running at 2.3 Ghz (turbo boost disabled)
and 128 GB memory. We dedicate 8 physical cores (16 Intel{\textregistered} hyper
threads) and 16 GB memory as cloudlet resources using cgroup. We run 8
experiments with increasing numbers of clients across four concurrent
applications. The total number of clients gradually increases from 15 to 40.
Table~\ref{tab:alloc-exps} shows the breakdown of the number of clients used for
each experiment. Note that these clients are running simultaneously, resulting
in heavier and heavier contention. We generate application adapation profiles
offline using the method discussed in Section~\ref{sec: resource-allocation}. We
leverage these profiles to optimize for maximizing the total system utility.
Figure~\ref{fig:alloc-max-util} shows how the total system utility changes as we
add more clients and hence more workload. The yellow line represents the
Original Gabriel which relies on the operating system alone to divide system
resources. The blue line shows our Scalable Gabriel approach. In the beginning,
while the system is under-utilized, we see that the Original Gabriel yields
slightly higher total utility. However, as contention increases, Original
Gabriel's total utility quickly drops, eventually more than 40\%, since every
client contends for resources in an uncontrolled fashion.  All applications
suffer, but the effects of increasing latencies are vastly different among
different applications. In contrast, scalable Gabriel maintains a high level of
system-wide utility by differentially allocating resources to different
applications based on their sensitivity captured in the adaptation profiles.

\begin{figure}[h]
  \centering
  \includegraphics[width=.9\linewidth]{FIGS/fig-alloc-max-util.pdf}
  \caption{Total Utility with Increasing Contention}
  \label{fig:alloc-max-util}
\end{figure}


Figure~\ref{fig:alloc-latency} and Figure~\ref{fig:alloc-fps} provide insights
into how scalable Gabriel strikes the balance. We present both application
throughput in terms of average frames per second and latency in terms of
90\%-tile response delay. Latencies are better controlled as resources are
dedicated to applications with high utility, and more clients are kept within
their latency bounds. Of course, with higher contention, fewer frames per second
can be processed for each client. Original Gabriel degrades applications in an
undifferentiated fashion. Scalable Gabriel, in contrast, tries to maintain
higher throughput for some applications at the expense of the others, e.g. LEGO
up to 27 clients. The accuracies of application profiles influence how well
Scalable Gabriel can manage latency. Run-time resource demand could deviate
from profiles due to the differences in the request content (e.g. image
content). Profile inaccuracies result in the overshoot of POOL and IKEA
90\%-tile latencies in Figure~\ref{fig:alloc-latency}, as the profiles
underestimate their resource demand and overestimate LEGO resource demand when
the number of clients is low. When the system becomes more crowded, the
throttling of LEGO throughput reduces such an effect.

\begin{figure}[]
  \begin{center}
    \includegraphics[width=\linewidth]{FIGS/fig-alloc-latency-legend.pdf}
    \includegraphics[width=\linewidth]{FIGS/fig-alloc-latency-baseline.pdf}
    {(a) Original Gabriel}
    \includegraphics[width=\linewidth]{FIGS/fig-alloc-latency-cpushares.pdf}
    {(b) Scalable Gabriel}
  \end{center}
  \begin{captiontext}
    \centering
    The normalization is  by per-application tight and loose
    bounds~\cite{chen2017empirical}.

    The allocation policy is to maximize the overall system utility.
  \end{captiontext}
  \caption{Normalized 90\%-tile Response Latency}
  \label{fig:alloc-latency}
\end{figure}

\begin{figure}[]
  \begin{center}
    \includegraphics[width=.9\linewidth]{FIGS/fig-alloc-latency-legend.pdf}
    \includegraphics[width=\linewidth]{FIGS/fig-alloc-fps-baseline.pdf}
    {(a) Original Gabriel}
    \includegraphics[width=\linewidth]{FIGS/fig-alloc-fps-cpushares.pdf}
    {(b) Scalable Gabriel}
  \end{center}
  \caption{Average Processed Frames Per Second Per Client}
  \label{fig:alloc-fps}
\end{figure}


\subsection{Effects on Guidance Latency}

We next evaluate the combined effects of workload reduction and
resource allocation in our system. We emulate many users running
multiple applications simultaneously. All users share the same
cloudlet with 8 physical cores and 16 GB memory. We conduct three experiments,
with 20 (4 clients per app), 30 (6 clients per app), and 40 (8 clients
per app) clients. Each client loops through pre-recorded video traces
with random starting points.  Figure~\ref{fig:frame-latency} and
Fig~\ref{fig:frame-fps} show per client frame latency and FPS
achieved. The first thing to notice is that concurrently utilizing
both sets of techniques does not cause conflicts. In fact, they appear
to be complementary and latencies remain in better control than using
resource allocation alone.

\begin{figure}[]
  \begin{center}
    \includegraphics[width=\linewidth]{FIGS/fig-alloc-latency-legend.pdf}

    \begin{tabular}{c@{}c}
      \includegraphics[width=.5\linewidth]{FIGS/fig-eval-latency-baseline.pdf}
                              & \includegraphics[width=.5\linewidth]{FIGS/fig-eval-latency-cpushares.pdf} \\
      {(a) Original  Gabriel} & {(b) Scalable Gabriel}
    \end{tabular}
  \end{center}

  \begin{captiontext}
    \centering
    The normalization is by per-application tight and loose
    bounds~\cite{chen2017empirical}.
  \end{captiontext}
  \vspace{-0.1in}
  \caption{Normalized 90\%-tile Response Latency}
  \label{fig:frame-latency}
\end{figure}

\begin{figure}[]
  \begin{center}
    % \includegraphics[width=\linewidth]{FIGS/fig-alloc-latency-legend.pdf}
    \begin{tabular}{c@{}c}
      \includegraphics[width=.5\linewidth]{FIGS/fig-eval-fps-baseline.pdf}
                             & \includegraphics[width=.5\linewidth]{FIGS/fig-eval-fps-cpushares.pdf} \\
      {(a) Original Gabriel} & {(b) Scalable Gabriel}
    \end{tabular}
  \end{center}
  \caption{Processed Frames Per Second Per Application}
  \label{fig:frame-fps}
\end{figure}

\begin{figure}[]
  \begin{center}
    \includegraphics[width=.7\linewidth]{FIGS/fig-alloc-latency-legend.pdf}
    \includegraphics[width=.7\linewidth]{FIGS/fig-sec6-latency-allocation.pdf}
  \end{center}
  \vspace{-0.1in}
  \caption{Fraction of Cloudlet Processing Allocated}
  \label{figs:resource-allocated}
\end{figure}

\begin{figure}[]
  \centering
  \includegraphics[width=0.5\linewidth, trim=0em 0em 0em 0em, clip]{FIGS/fig-sec6-latency-legend.pdf} \\
  \centering
  \begin{turn}{90}{\hspace{0.6in}\small (a) FACE}\end{turn}\hspace{0.2in}\includegraphics[width=2.5in, trim=0em 0em 0em 0em, clip]{FIGS/fig-sec6-latency-face.pdf}
  \begin{turn}{90}{\hspace{0.6in}\small (b)
      LEGO}\end{turn}\hspace{0.2in}\includegraphics[width=2.5in, trim=0em 0em 0em 0em,
    clip]{FIGS/fig-sec6-latency-lego.pdf}\\[0.08in]
  \vspace{0in}
  \begin{turn}{90}{\hspace{0.6in}\small (c) PING PONG}\end{turn}\hspace{0.2in}\includegraphics[width=2.5in, trim=0em 0em 0em 0em, clip]{FIGS/fig-sec6-latency-pingpong.pdf}
  \begin{turn}{90}{\hspace{0.6in}\small (d)
      POOL}\end{turn}\hspace{0.2in}\includegraphics[width=2.5in, trim=0em 0em 0em 0em,
    clip]{FIGS/fig-sec6-latency-pool.pdf}\\[0.08in]
  \vspace{0in}
  \begin{turn}{90}{\hspace{0.6in}\small (e) IKEA}\end{turn}\hspace{0.2in}\includegraphics[width=2.5in, trim=0em 0em 0em 0em, clip]{FIGS/fig-sec6-latency-ikea.pdf}
  \caption{Guidance Latency Compared to Loose Latency Bound}
  \label{figs:inst-delay}
\end{figure}

The previous plots consider per request latencies. The ultimate goal of our work
is to maintain user experience as much as possible and degrade it gracefully
when overloaded. For WCA applications, the key measure of user experience is
guidance latency, the time between the occurrence of an event and the delivery
of corresponding guidance. Note that guidance latency is different than per
request latency, as a guidance may need not one but several frames to recognize
a user state. Figure~\ref{figs:inst-delay} shows boxplots of per-application
guidance latency for the concurrent application experiments above. The red
dotted line denotes the application-required loose bound. It is clear that our
methods control latency significantly better than the baseline. Scalable Gabriel
is able to serve at least 3x number of clients when moderately loaded while
continuing to serve more than half of the clients when severely loaded. In these
experiments, the utility is maximized at the expense of the FACE application,
which provides the least utility per resource consumed. At the highest number of
clients, scalable Gabriel sacrifices the LEGO application to maintain the
quality of service for PINGPONG and POOL. This differentiated allocation is
reflected in Figure~\ref{figs:resource-allocated}. In contrast, with original
Gabriel, none of the applications are able to regularly meet deadlines.