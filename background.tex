\chapter{Background}
\label{chapter: background}

\section{Edge Computing}

\begin{figure*}[t]
\begin{minipage}[b]{4.3in}
\begin{minipage}[c]{1.68in}
\includegraphics[scale=0.3]{FIGS/fig-3tier-A.pdf}
\end{minipage}
\begin{minipage}[c]{1.75in}
\vspace{0.01in}
\includegraphics[scale=0.16]{FIGS/fig-3tier-B-cropped.pdf}\\
\end{minipage}
\begin{minipage}[c]{0.55in}
\includegraphics[scale=0.33]{FIGS/fig-3tier-C.pdf}
\end{minipage}
\caption{\small Tiered Model of Computing}
\label{fig:3tier}
\end{minipage}
\end{figure*}

{\em Edge computing} is a nascent computing paradigm which has gained
significant traction over the past few years. It champions the vision to place
substantial compute and storage resources at the edge of the Internet, in close
proximity to mobile devices or sensors.  Terms such as
``cloudlets''~\cite{Satya2009}, ``micro data centers (MDCs)''~\cite{Greene2012},
``fog''~\cite{Bonomi2012}, and ``mobile edge computing (MEC)''~\cite{Brown2013}
are used to refer to these small, edge-located computing nodes.  We use these
terms interchangably in the rest of this thesis. Edge computing is motivated by
its potential to improve latency, bandwidth, and scalability over a cloud-only
model.  More practically, some efforts stem from the drive towards
software-defined networking (SDN) and network function virtualization (NFV), and
the fact that the same hardware can provide SDN, NFV, and edge computing
services. This suggests that infrastructure providing edge computing services
may soon become ubiquitous, and may be deployed at greater densities than
content delivery network (CDN) nodes today. 

Satya et al.~\cite{satya2019computing} best describes the modern computing
landscape with edge computing using a tiered model, shown in
Figure~\ref{fig:3tier}. Tiers are separated by distinct yet stable sets of
design constraints. From left to right, this tiered model represents a hierarchy
of increasing physical size, compute power, energy usage, and elasticity. Tier-1
represents today's large-scale and heavily consolidated data-centers. Compute
elasticity and storage permanence are two dominating themes here. Tier-3
represents IoT and mobile devices, which are constrained by their physical size,
weight, and heat dissipation. Sensing is the key functionality of tier-3
devices. Today's mobile devices are already rich in sensors, including camera,
microphone, accelerometers, gyroscopes and GPS. In addition, an increasing
amount of IoT devices are getting adopted, e.g. smart speakers, security
cameras, smart thermostats. However, there often exists a tension between the
amount of data generated at tier-3 devices and their capabilities to process
these data. To overcome this tension, a tier-3 device could offload computation
over network to tier-1. This capability was first demonstrated in 1997 by Noble
et al.~\cite{Noble1997}, who used it to implement speech recognition with
acceptable performance on a resource-limited mobile device. In 1999, Flinn et
al.~\cite{Flinn1999} extended this approach to improve battery life.  These
concepts were generalized in a 2001 paper that introduced the term {\em cyber
foraging} for the amplification of a mobile device's data or compute
capabilities by leveraging nearby infrastructure~\cite{Satya2001}.  

However, today's datacenter is no longer ``near`` tier-3 devices due to
the consolidation needed to achieve the economy of scale. Li et
al.~\cite{li2010cloudcmp} report that average round trip time (RTT) from 260
global vantage points to their optimal Amazon EC2 instances is 74 ms. To make it
worse, the high network fan-in of datacenter means its aggregation network would
face significant challenges to handle the ever-increasing volume of ingress
traffic caused by the exponential growth of data at tier-3. 

\begin{figure}
\centering
\includegraphics[height=1.5in]{FIGS/ping_cdf.pdf}
\caption{CDF of pinging RTTs}\label{fig:ping-CDF}
\end{figure}

To address these problems, cloudlets at tier-2 comes into the place to create
the illusion of bringing Tier-1 ``closer``. It serves two purposes. First,
cloudlets provide infrastructure for compute-intensive and latency-sensitive
applications at tier-3. Wearable cognitive assistance is an excellent example of
these applications. Second, by processing data closer to the source of the
content, it reduces the excessive traffic going into tier-1 datacenters.
Figure~\ref{fig:ping-CDF} shows the RTT comparison of PING to the cloud and the
cloudlet over WiFi and LTE. Cloudlet's RTT is on average 80 to 100ms shorter
than its counterpart to the cloud.

The low-latency and high-bandwidth compute infrastructure provided by cloudlets
is the foundation of creating wearable cognitive assistance. It also poses
unique challenges when scaling WCAs to many users, which are the key questions
this thesis set out to investigate. 


\section{Gabriel Platform}
\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{FIGS/fig-backend-structure-simple-crop.pdf}
\begin{captiontext}
{\rm (Source: Chen et al~\cite{chen2017empirical})}
\end{captiontext}
\caption{\small Gabriel Platform}
\label{fig:gabriel}
\end{figure}

The Gabriel platform~\cite{ha2014towards,chen2017empirical}, shown in
Figure~\ref{fig:gabriel}, is the first implementation of WCA and provides an
application framework to simplify development. The Gabriel front-end on a
wearable device performs preprocessing of sensor data (e.g., compression and
encoding), which it streams over a wireless network to a cloudlet.  The Gabriel
back-end on the cloudlet has a modular structure. The {\em control module} is
the focal point for all interactions with the wearable device.  A
publish-subscribe (PubSub) mechanism decodes and distributes the incoming sensor
streams to multiple {\em cognitive modules} (e.g., task-specific computer vision
algorithms) for concurrent processing. Cognitive module outputs are integrated
by a task-specific {\em user guidance module} that performs higher-level
cognitive processing such as inferring task state, detecting errors, and
generating guidance in one or more modalities (e.g., audio, video, text, etc.).

The original Gabriel platform was built with a single user in mind,
and did not have mechanisms to share cloudlet resources in a
controlled manner.  It did, however, have a token-based transmission
mechanism.  This limited a client to only a small number of
outstanding operations, thereby offering a simple form of rate
adaptation to processing or network bottlenecks.  We have retained
this token mechanism in our system, described in the rest of this paper.
In addition, we have extended Gabriel with new mechanisms to handle
multitenancy, perform resource allocation, and support
application-aware adaptation.  We refer to the two versions of the
platform as ``Original Gabriel'' and ``Scalable Gabriel.''

\section{Example Gabriel Applications}

Many applications have been built on top of the Gabriel platform.  Recent
papers~\cite{chen2017empirical}~\cite{chen2018application} describe these applications,
along with detailed analysis of their end-to-end latency. For example, the LEGO
application guides a user to construct a Lego model, step by step, continuously
monitoring the task with computer vision, and providing instructions when it has
detected that the user has completed a step. POOL assists a user in aiming a
pool cue stick. PING PONG suggests hitting a ball to the left or right to win a
rally in table tennis. FACE recognizes a face that has appeared in a scene,
searches the user's personal database, and whispers the person's name. IKEA
helps a user to assemble an IKEA lamp step by step.

These applications run on multiple wearable devices such as Google
Glass, Microsoft HoloLens, Vuzix Glass, and ODG R7. At a high level, 
the cloudlet workflows of these applications are similar, and consist of
two major phases.  The first phase uses computer vision
to extract a symbolic, idealized representation of the state of the
task, accounting for real-world variations in lighting, viewpoint,
etc.  The second phase operates on the symbolic representation,
implements the logic of the task at hand, and occasionally generates
guidance for the user.  In most WCA applications, the first phase is
far more compute intensive than the second phase.

