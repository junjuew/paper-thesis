\chapter{Background}
\label{chapter: background}

\section{Wearable Cognitive Assistance}

Amplifying human cognition in real time through low-latency wireless
access from wearable devices to infrastructure resources was first
presented as science fiction in 2004~\cite{Satya2004}.  The building
blocks for this vision came into place by 2014, enabling the first
implementation of this concept in {\em Gabriel}~\cite{Ha2014}. In
2017, Chen et al~\cite{Chen2017} described a number of applications of
this genre, quantified their latency requirements, and profiled the
end-to-end latencies of their implementations.  In late 2017, SEMATECH
and DARPA jointly funded \$27.5 million of research on such
applications~\cite{Oakley2018, Stokes2018}.  At the Mobile World
Congress in February 2018, wearable cognitive assistance was the focus
of an entire session~\cite{Ray2018}.  For AI-based military use cases,
this class of applications is the centerpiece of ``Battlefield
2.0''~\cite{Doffman2018}.  By mid-2019, WCA was being viewed as a prime source of ``killer
apps'' for edge computing~\cite{Satya2019b,Satya2019c}.

\section{Gabriel Platform}

Our work is built on the Gabriel platform~\cite{Ha2014,Chen2017},
shown in Figure~\ref{fig:gabriel}.  The Gabriel front-end on a
wearable device performs preprocessing of sensor data (e.g.,
compression and encoding), which it streams over a wireless network
to a cloudlet.  The Gabriel back-end on the cloudlet has a modular structure.
The {\em control module} is the
focal point for all interactions with the wearable device.  A
publish-subscribe (PubSub) mechanism decodes and distributes the
incoming sensor streams to multiple {\em cognitive modules} (e.g.,
task-specific computer vision algorithms) for concurrent processing.
Cognitive module outputs are integrated by a task-specific {\em user
  guidance module} that performs higher-level cognitive processing
such as inferring task state, detecting errors, and
generating guidance in one or more modalities (e.g., audio, video, text, etc.).

The original Gabriel platform was built with a single user in mind,
and did not have mechanisms to share cloudlet resources in a
controlled manner.  It did, however, have a token-based transmission
mechanism.  This limited a client to only a small number of
outstanding operations, thereby offering a simple form of rate
adaptation to processing or network bottlenecks.  We have retained
this token mechanism in our system, described in the rest of this paper.
In addition, we have extended Gabriel with new mechanisms to handle
multitenancy, perform resource allocation, and support
application-aware adaptation.  We refer to the two versions of the
platform as ``Original Gabriel'' and ``Scalable Gabriel.''

\section{Example Gabriel Applications}

Many applications have been built on top of the Gabriel platform.  Recent
papers~\cite{Chen2017}~\cite{chen2018} describe these applications, along with detailed analysis
of their end-to-end latency. For example, the LEGO application guides a user to
construct a Lego model, step by step, continuously monitoring the task with
computer vision, and providing instructions when it has detected that the user
has completed a step. POOL assists a user in aiming a pool cue stick. PING PONG
suggests hitting a ball to the left or right to win a rally in table tennis.
FACE recognizes a face that has appeared in a scene, searches the user's
personal database, and whispers the person's name. IKEA helps a user to
assemble an IKEA lamp step by step.

These applications run on multiple wearable devices such as Google
Glass, Microsoft HoloLens, Vuzix Glass, and ODG R7. At a high level, 
the cloudlet workflows of these applications are similar, and consist of
two major phases.  The first phase uses computer vision
to extract a symbolic, idealized representation of the state of the
task, accounting for real-world variations in lighting, viewpoint,
etc.  The second phase operates on the symbolic representation,
implements the logic of the task at hand, and occasionally generates
guidance for the user.  In most WCA applications, the first phase is
far more compute intensive than the second phase.

