\section{Video Processing on Mobile Devices}
\label{bw:challenges}

In the context of real-time video analytics, Tier-3 devices represent
fundamental mobile computing challenges that were identified two decades
ago~\cite{Satya1996}.  Two challenges have specific relevance here. First,
mobile elements are resource-poor relative to static elements.  Second, mobile
connectivity is highly variable in performance and reliability.  We discuss
their implications below.


\subsection{Computation Power on Tier-3 Devices}
\label{bw:payload}

Unfortunately, the hardware needed for deep video stream processing in real time
is larger and heavier than can fit on a typical Tier-3 device. State-of-art
techniques in image processing use DNNs that are compute- and memory-intensive.
Figure~\ref{fig:onboard-dnn-speed} presents experimental results on two
fundamental computer vision tasks, image classification and object detection, on
four different devices. In the figure, MobileNet V1 and ResNet101 V1 are image
classification DNNs. Others are object detection DNNs. Both tasks used publicly
available pretrained DNN models. We carefully choose hardware platforms to
represent a range of computation capabilities of Tier-3 devices. To anticipate
future improvements in smartphone technology, our experiments also consider more
powerful devices such as the Intel$^{\tiny\textregistered}$
Joule~\cite{Hardawar2016} and the NVIDIA Jetson~\cite{NVIDIA2017} that are
physically compact and light enough to be credible as wearable device platforms
in a few years.

Fig.~\ref{fig:onboard-dnn-speed}, we present the best results we could obtain on
each platform. This is not intended to directly compare frameworks and platforms
(as others have been doing~\cite{Zhang2018pcamp}), but rather to illustrate the
differences between wearable platforms and fixed infrastructure servers. 

\begin{figure}
\centering
\begin{flushleft}
M: MobileNet V1; R: ResNet101 V1;
S-M: SSD MobileNet V1; S-I: SSD Inception V2;\\F-I: Faster R-CNN Inception V2;
F-R: Faster R-CNN ResNet101 V1
\end{flushleft}
\sisetup{
    table-format=3,
    table-number-alignment=left
}
\hspace{-0.8in}
\begin{tabular}{|p{1.3cm}|S[table-column-width=1.3cm]|p{3.0cm}|p{1.8cm}|S[table-column-width=1.2cm]|S[table-column-width=1.4cm]|S[table-column-width=1.3cm]|S[table-column-width=1.3cm]|S[table-format=4, table-number-alignment=left, table-column-width=1.5cm]|S[table-format=5, table-number-alignment=left, table-column-width=1.9cm]|}
\hline
{\multirow{2}{*}{}} & {\multirow{2}{*}{}} & {\multirow{2}{*}{}}                                        & {\multirow{2}{*}{}}                                                                        & \multicolumn{2}{c|}{\parbox[t]{1.8cm}{\centering Image\\Classification\\~}}                                & \multicolumn{4}{c|}{Object Detection} \\ \cline{5-10}
                  &  {\parbox[t]{0.9cm}{\centering Weight\\(g)}}
                  & \centering CPU
                  & \centering GPU
                  & {\parbox[t]{0.9cm}{\centering M\\(ms)}}
                  & {\parbox[t]{1.1cm}{\centering R\\(ms)}}
                  & {\parbox[t]{1.0cm}{\centering S-M\\(ms)}}
                  & {\parbox[t]{1.0cm}{\centering S-I\\(ms)}}
                  & {\parbox[t]{1.3cm}{\centering F-I\\(ms)}}
                  & {\parbox[t]{1.6cm}{\centering F-R\\(ms)}} \\ \hline
Nexus~6    & 184                                                      & 4-core 2.7 GHZ Krait 450, 3GB Mem                           & Adreno 420                                                                              & 353 {\scriptsize \ (67)}                                                 & 983 {\footnotesize \ (141)}                                                   & 441 {\footnotesize \ (60)           }                               & 794 {\footnotesize \ (44)            }                                       & {\small ENOMEM}                                                                 & {\small ENOMEM}                                                               \\ \hline
Intel$^{\tiny \textregistered}$ Joule 570x     & 25                                                       & 4-core 1.7 GHz Intel Atom$^{\tiny\textregistered}$ T5700, 4GB Mem                          & Intel$^{\tiny\textregistered}$ HD Graphics (gen 9)                                                               & 37 {\scriptsize \ (1)$\ddagger$ }                                       & 183 {\footnotesize \ (2)$\dagger$$\ddagger$ }                                        & 73  {\footnotesize \ (2)$\ddagger$ }                               & 442 {\footnotesize \ (29)            }                                       & 5125 {\footnotesize \ (750)}                                                             & 9810 {\footnotesize \ (1100)}                                                          \\ \hline
NVIDIA Jetson TX2     & 85                                                       & 2-Core 2.0 GHz Denver2 + 4-Core 2.0 GHz Cortex-A57, 8GB Mem & 256 cuda core 1.3 GHz NVIDIA Pascal                                                          & 13 {\scriptsize \ (0)$\dagger$  }                                       & 92 {\footnotesize \ (2)$\dagger$ }                                          & 192 {\footnotesize \ (18)           }                               & 285 {\footnotesize \ (7)$\dagger$   }                                       & {\small ENOMEM}                                                                 & {\small ENOMEM}                                                               \\ \hline
Rack-mounted Server     &                                                          & 2x 36-core 2.3 GHz Intel$^{\tiny\textregistered}$ Xeon$^{\tiny\textregistered}$ E5-2699v3 Processors, 128GB Mem     & 2880 cuda core 875MHz NVIDIA Tesla K40, 12GB GPU Mem                             & 4 {\scriptsize \ (0)$\ddagger$  }                                       & 33 {\footnotesize \ (0)$\dagger$  }                                          & 12  {\footnotesize \ (2)$\ddagger$ }                               & 70  {\footnotesize \ (6)             }                                       & 229 {\footnotesize \ (4)$\dagger$}                                                               & 438 {\footnotesize \ (5)$\dagger$}                                                               \\ \hline
\end{tabular}
\begin{captiontext}
\vspace{0.1in}
Figures above are means of 3 runs across 100 random images. The time shown
includes only the forward pass time using batch size of 1. ENOMEM indicates
failure due to insufficient memory. Figures in parentheses are standard
deviations. The weight figures for Joule and Jetson include only the modules
without breakout boards. Weight for Nexus~6 includes the complete phone with
battery and screen. Numbers are obtained with TensorFlow (TensorFlow Lite for
Nexus 6) unless indicated otherwise. \\
$\dagger$ indicates GPU is used. $\ddagger$ indicates
Intel$^{\tiny\textregistered}$ Computer Vision SDK beta 3 is used.
\end{captiontext}
\caption{Deep Neural Network Inference Speed}
\label{fig:onboard-dnn-speed}
\end{figure}

Image classification maps an image into categories, with each category
indicating whether one or many particular objects (e.g., a human survivor, a
specific animal, or a car) exist in the image.  The prediction speed using two
different DNNs are shown. MobileNet V1~\cite{Howard2017} is a DNN
designed for mobile devices from the ground-up by reducing the number of
parameters and simplifying the computation using
depth-wise separable convolution. ResNet101 V1~\cite{He2016} is a more accurate
but also more resource-hungry DNN that won the ImageNet
classification challenge in 2015~\cite{Russakovsky15}. 

Object detection is a harder task than image classification, because it requires
bounding boxes to be predicted around the specific areas of an image that
contains a particular class of object. Object detection DNNs are built on top of
image classification DNNs by using image classification DNNs as low-level
feature extractors. Since feature extractors in object detection DNNs can be
changed, the DNN structures excluding feature detectors are referred as object
detection meta-architectures. We benchmarked two object detection DNN
meta-architectures: Single Shot Multibox Detector (SSD)~\cite{Liu2016} and
Faster R-CNN~\cite{Ren2015}. We used multiple feature extractors for each
meta-architecture. The meta-architecture SSD uses simpler methods to identify
potential regions for objects and therefore requires less computation and runs
faster. On the other hand, Faster R-CNN~\cite{Ren2015} uses a separate region
proposal neural network to predict regions of interest and has been shown to
achieve higher accuracy~\cite{Huang2017}. Figure~\ref{fig:onboard-dnn-speed}
presents results in four columns: SSD combined with MobileNet V1 or Inception
V2, and Faster R-CNN combined with Inception V2 or ResNet101 V1~\cite{He2016}.
The combination of Faster R-CNN and ResNet101 V1 is one of the most accurate
object detectors available today~\cite{Russakovsky15}. The entries marked ``{\sc
ENOMEM}'' correspond to experiments that were aborted because of insufficient
memory.

These results demonstrates the computation gap between mobile and static
elements. While the most accurate object detection model Faster R-CNN Resnet101 V1
can achieve more than two FPS on a server GPU, it either takes several seconds
on  mobile platforms or fails to execute due to insufficient memory. In
addition, the figure also confirms that sustaining open-ended real-time video
analytics on smartphone form factor computing devices is well beyond the state
of the art today and may remain so in the near future.  This constrains what is
achievable with Tier-3 devices.

\subsection{Result Latency, Offloading \& Scalability}
\label{bw:offloading}

{\em Result latency} is the delay between first capture of a video frame in
which a particular result (e.g., image of a survivor) is present, and report of
its discovery or feedback based on the discovery after video processing.
Operating totally disconnected, a Tier-3 device can capture and store video, but
defer its processing until the mission is complete.  At that point, the data can
be uploaded from the device to the cloud and processed there.  This approach
completely eliminates the need for real-time video processing, obviating the
challenges of Tier-3 computation power mentioned previously. Unfortunately, this
approach delays the discovery and use of knowledge in the captured data by a
substantial amount (e.g., many tens of minutes to a few hours).  Such delay may
be unacceptable in use cases such as search-and-rescue using drones, or
step-by-step instruction feedback from wearable devices. In this chapter, we focus
on approaches that aim for much smaller result latency: ideally, close to
real-time.

A different approach is to offload video processing during flight over a
wireless link to an edge computing node (cloudlet). With this approach, even a
weak Tier-3 device can leverage the substantial processing capability of a
ground-located cloudlet, without concern for its weight, size, heat dissipation,
or energy usage.  Much lower result latency is now possible.  However, even if
cloudlet resources are viewed as ``free'' from the viewpoint of mobile
computing, the Tier-3 device consumes wireless bandwidth in transmitting video.

Today, 4G LTE offers the most plausible wide-area connectivity from a Tier-3
device to its associated cloudlet.  The much higher bandwidths of 5G are still
many years away, especially at global scale.  More specialized wireless
technologies, such as Lightbridge 2~\cite{LightBridge2} for drones, can also be
used.  Regardless of specific wireless technology, the principles and techniques
described in this chapter apply.

{\em Scalability,} in terms of maximum number of concurrently operating Tier-3
devices within a 4G LTE cell becomes an important metric.  In this chapter we
explore how the limited processing capability on a Tier-3 device can be used to
greatly decrease the volume of data transmitted, thus improving scalability
while minimally impacting result accuracy and result latency.

Note that the uplink capacity of 500 Mbps per 4G LTE cell assumes standard
cellular infrastructure that is undamaged.  In natural disasters and military
combat, this infrastructure may be destroyed. Emergency substitute
infrastructure, such as Google and AT\&T's partnership on balloon-based 4G LTE
infrastructure for Puerto Rico after hurricane Maria~\cite{Morse2017}, can only
sustain much lower uplink bandwidth per cell, e.g. 10Mbps for the balloon-based
LTE~\cite{Sankaran2018}.  Conserving wireless bandwidth from Tier-3 video
transmission then becomes even more important, and the techniques described here
will be even more valuable.

% \emph{Result accuracy} influences a second dimension of scalability, namely
% the ability of one individual to supervise the result streams from
% many drones.  The output of each video processing pipeline should only
% demand occasional human attention.  The accuracy, sophistication, and
% speed of this pipeline determines the cognitive load on mission
% personnel for a given video stream.  For example, a pipeline that has
% virtually no false positives or false negatives in detecting survivors
% will consume less supervisory human attention than a mediocre
% pipeline.  That will allow one person to confidently supervise a large
% swarm that rapidly covers a large search area.
