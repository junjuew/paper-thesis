\chapter{Wearable Cognitive Assistance Development Tools}
\label{chapter: app-dev}

While previous chapters address the system challenges of scaling wearable
cognitive assistance at the edge, another key obstacle to the widespread
adoption of WCAs is the level of specialized skills and the long development
time needed to create new applications. Such expertise includes task domain
knowledge, networked application development skills, and computer vision
insights. Researchers~\cite{chen2018application} report that it typically takes
multiple person-months of effort to create a single application. The majority of
development time is spent on learning new computer vision techniques, selecting
robust algorithms to use through trial and error, and implementing the
application. The high barrier to entry significantly limits the number of
wearable cognitive assistants available today. Clearly, this is not scalable. 

In this chapter, we reflect on the existing ad hoc WCA development process,
propose a new development methodology centered around DNN-based object
detection, and present development tools that we have built to lower the barrier
of WCA development. Our goal is to simplify the process so that a small team
(1-2 people) of a task expert and a developer, without computer vision
expertise, can create an initial version of a Gabriel application within a few
days. This is a productivity improvement of at least one order of magnitude.
Refinement and tuning may take longer, but can be guided by the early use of the
application. 

Simplification is difficult. The application needs a precise definition of the
end-point state of each step (e.g., a particular screw mounted into a
workpiece), yet needs to be tolerant of alternative paths to reaching that end
point (e.g., hand-tightening versus using a screwdriver). We assume the small
development team has knowledge of the task specifics and general programming
skills, but not necessarily experience with wearable devices or computer vision.

\section{Ad Hoc WCA Development Process}
\label{sec: app-dev-adhoc}

\begin{figure}
  \centering
  \includegraphics[trim={0 6cm 0 0},width=\linewidth]{FIGS/ad-hoc-workflow}
	\caption{Ad Hoc Development Workflow}
    \label{figs:workflow}
\end{figure}

The existing ad hoc development process of WCAs can be described as
figure~\ref{figs:workflow}. Developers first work with the task expert to
identify and pinpoint a specific use case. With the use case in mind, the
development team needs to identify critical visual features and states that can
be reliably recognized with machine vision. In the meantime, the use case is
broken down into individual steps. For each step, feedback messages to users are
created based on detected visual states. Potential errors are also enumerated
and included as additional steps. We refer to the complete set of the steps
annotated with visual states and feedback as the \textit{Task Model}. For
example, for the LEGO wearable cognitive assistance~\cite{chen2017empirical},
the task model contains the sequence of blocks to build up the final assembled
Lego piece, together with potentially misused blocks. The visual states to
recognize are the pieces currently on the lego board: the shape and the color of
individual lego block, and their composed shape and color.

Notably, a task model is not only determined by the task itself, but also
influenced heavily by visual states that can be reliably detected. In fact, it
is common to alter the sequence of steps or introduce additional steps to reduce
the difficulties and improve the reliability of computer vision recognition.
Since there is a human in the loop (the user), relying on humans to do what they
are good at is the main reason that wearable cognitive assistance can be
implemented without solving the generic perception and planning problems
intrinsic to robotics. For example, a frequently used technique is to ask the
user to hold the object of interests at a fixed viewpoint shown in the image
guidance. Narrowing the viewing angle makes the recognition problem more
tractable.

The application task model serves as a blueprint for implementation. For each step
in the task model, developers select and implement computer vision algorithms.
Custom logic is also written to handle step transitions and interface with the
Gabriel platform. After initial implementation, test runs are conducted to
evaluate the robustness of computer vision and measure the end-to-end
application latency. The implementation process is typically iterative to allow
trials and errors when choosing computer vision algorithms.

The ad hoc development process has unnecessary high requirements and burden for
WCA creators in the following aspects.
\begin{enumerate}
  \item Developers need to be familiar with various computer vision algorithms
to determine what visual states can be recognized reliably. The knowledge of
selecting algorithms for specific objects, environments, and setups requires
years of experience. This high bar of entry hinders the creation of WCAs.
  \item It takes a significant amount of time to implement computer vision
algorithms in a trial and error fashion. Several months of coding time could
turn into a fruitless outcome when the algorithm turns out to be a bad fit. For
sophisticated WCAs with tens or hundreds of visual states, such a trial and
error methodology is not scalable. Needed are unified and automated methods to
create visual state recognizers.
  \item The tight coupling of task model design and visual state recognition
calls for deep understanding in both task domain and computer vision. When
visual states in the task model end up being too difficult for even the
state-of-art algorithms, the development team needs to adjust the task model to
either use alternative visual states or employ user assistance. For instance, in
the RibLoc application, during development, a task step was changed to asking
the user to read out a word on the gauge instead of performing optical character
recognition on the lightly-colored characters. Close collaborations among task
experts and developers are needed due to this tight intertwinement between task
model design and computer vision algorithm exploration. Methodologies and tools
that shorten the turn-around time when a task model changes can significantly
reduce the overall development time.
\end{enumerate}


% Among all the development procedures, creating the computer vision checks to
% detect user states consumes the most of development time and requires computer
% vision expertise and experience. With the adoption of DNNs, developers no longer
% need to spend days to select and tweak handcrafted features. Instead, the entire
% model is trained end-to-end using labeled data. However, DNNs, with millions of
% parameters to train, requires a significant amount of training data. Collecting
% and labeling data are time-consuming and painstaking. Besides, to craft and
% implement a DNN by hand is not trivial. Significant machine learning background
% is needed to tweak network architectures and parameters. Therefore, developer
% tools are needed to both help label the data and create deep neural networks.

% In summary, implementing the workflow of cognitive assistance takes time and
% efforts. Ad-hoc implementation requires a team of domain experts, developers and
% computer vision experts. Such development model cannot scale to thousands of
% applications. Therefore, Gabriel needs to be extended with tools to reduce the
% effort of creating wearable cognitive assistants.

% Existing ad-hoc approach to develop wearable cognitive assistance not only takes
% a long time, but also requires significant amount of computer vision expertise.
% Developers new to wearable cognitive assistance would need to spend months
% learning computer vision basics and acquire intuitions to determine what is
% achievable before developing an application. 

% Figure~\ref{fig:workflow} shows the ad-hoc development process. 

\section{A Fast Prototyping Methodology}

To streamline the development process and lower the barrier of entry, we propose
a new prototype methodology which focuses on the following two aspects.
\begin{enumerate}
  \item  Use deep neural network (DNN) based object detection as the universal
  building block for visual state recognition.
  \item Use finite state machine to represent and implement application task model.
\end{enumerate}
We first introduce the concepts of this methodology and describe its benefits in
this section and then introduce tools we have built to automate the development
process in subsequent sections.

\subsection{Objects as the Universal Building Blocks}

To replace manual selection of computer vision algorithms through trial and
error, we propose to use DNN-based object detection as the fundamental building
block to detect user states. Using object detection as the universal building
block means that each visual state should be decomposed and described as a
collection of individual objects along with their attributes and relationships.
We argue that objects as fundamental building blocks provide expressive and
flexible constructions capable of describing a wide range of visual user states
important to WCAs. Intuitively, the addition and removal of objects from a scene
is straightforward to express. Spatial relationships among objects (e.g. close
to, on the left of, overlap significantly) capture semantic visual features that
can be used to infer user states. For example, the detection of two Ikea
furniture pieces close to each other could indicate that they have been
assembled together. Moreover, temporal relationships can add confidence to the
inference made from spatial relationship. The detection of two pieces of
furniture closely connected to each other in the last 30 frames implies strongly
that they have been assembled together. Of course, there are limitations on how
reliable the inference could be. Nonetheless, objects together with their
spatial and temporal relationships with one another, provide a flexible
reasoning framework to decompose user states into visual states.

\begin{figure}
  \centering
  \includegraphics[trim={0 0 0 0},width=.9\linewidth]{FIGS/pingpong}
	\caption{A Frame Seen by the PING PONG Assistant}
    \label{figs:pingpong-frame}
\end{figure}

To illustrate the effectiveness of this object centered approach, we showcase
how existing applications built without this methodology in mind can be easily
expressed. Figure~\ref{figs:pingpong-frame} shows an image that will trigger the
PING PONG assistance to provide an instruction to hit the ball to the left. We
can express this visual state with a collection of objects, namely the Ping-Pong
table, the ball, and the opponent. These objects should satisfy the following
spatial relationship for the instruction ``hit to the LEFT'' to be generated. To
improve the accuracy of our instruction, we can require the spatial relationship
of these objects to be held true for at least two consecutive frames before an
instruction is produced.

\begin{itemize}
  \item The Ping-Pong ball is above the Ping-Pong table for an active rally.
  \item The opponent is on the right side of the Ping-Pong table.
  \item The ball is also on the right side of the Ping-Pong table.
\end{itemize}

\begin{figure}
  \centering
  \includegraphics[trim={0 0 0 0},width=.9\linewidth]{FIGS/lego}
	\caption{A Frame Seen by the LEGO Assistant}
    \label{fig:lego-image}
\end{figure}

Similarly, we can express the user states using objects for another
application LEGO. Figure~\ref{fig:lego-image} shows a visual state for the
assembled pieces. Here the collection of objects needs to be present are two
blue Lego blocks, two black blocks, one white block, and one yellow block. The
spatial relationships among these block include:

\begin{itemize}
  \item The yellow block should be at the bottom and longer than all other blocks.
  \item The white block should be in the middle of a blue block and the yellow
  block. It should have the same width as the blue block.
  \item Two black blocks should be on the left of a blue block and the white
  block. It should also be on the bottom of a blue block.
  \item One blue bock should be at the top of all other pieces.
\end{itemize}

One challenge with object detection is to obtain the absolute scale. In this
example, it would be difficult to get the exact width of the lego block (e.g.
whether it is a 1x4 or 1x3 lego piece) with object detection alone. However,
once the location of object is detected, additional processing can be used to
identify object attributes. For example, we can leverage the dots
on the lego board to calculate the absolute size of lego pieces.

The key benefit of using object detection as the universal unit to recognize
visual states is the possibility of automation. In recent years, deep neural
networks have dramatically increased the accuracy of object
detection~\cite{zou2019object}. In 2008, the best object detector, based on
deformable part model~\cite{felzenszwalb2008discriminatively}, achieved a mean
average precision (mAP) of 21\% on the Pascal Visual Object Classes
dataset~\cite{everingham2010pascal}. In less than ten years, deep neural
networks~\cite{he2017mask,Ren2015,He2016,lin2017focal} have quadrupled the
accuracy to 84\% on the same dataset. For a wide range of real-world objects,
DNNs have shown to be effective in both accurate identification of classes and
localization of object bounding boxes.  They can even differentiate subtly
different classes, such as breeds of dogs. Many commercial products now offer
features using DNN object detection. For example, Google
Photos~\cite{googlePhoto} and Pinterest Visual Search~\cite{pinterest} allow
users to search for images containing object of interest using text.

In addition to significant improvement in accuracy, DNNs also provide a unified
method to create object detectors. Modern DNN-based object detection employs an
end-to-end learning approach, in which one or multiple deep neural networks are
trained to predict the object classes and locations directly from RGB images.
Gone is the need to hand craft features. Instead, DNNs, on their
own, find distinguishable features during the training process as they are
presented with labeled examples of different objects. The substitution of custom
CV with machine-learned models makes automation possible. A typical DNN training
process consists of the following steps.

\begin{enumerate}
  \item Collect images of object of interests from diverse background,
  perspectives, and lighting conditions.
  \item Annotate these images with bounding boxes and class names to create a
  dataset of training data, evaluation data, and test data.
  \item Implement a DNN-based object detection network, typically using machine learning frameworks,
  such as Tensorflow~\cite{abadi2016tensorflow} and
  PyTorch~\cite{paszke2019pytorch}.
  \item Continuously train and evaluate a DNN model using the labeled dataset.
  \item Test the accuracy of the trained DNN model on the test data.
\end{enumerate}

While a unified training method eliminates manual feature engineering, creating
a DNN-based object detector is still both time-consuming and painstaking for the
following reasons. First, DNNs often have millions of model parameters and
therefore requires millions of labeled examples to train from scratch.
Collecting and labeling such large amount of training data takes significant
manual labor. Second, implementing a DNN correctly for object detection is not
trivial and still requires significant amount of knowledge of machine learning.
For example, state-of-art object detectors uses custom layers different from the
standard convolutional layer for better performance. Data augmentation, batch
normalization, and drop out are needed at training time for better results
through optimization, but should be disabled during inference time. To
streamline the process of creating DNN-based object detectors, we provide a web
application OpenTPOD (Section~\ref{sec: app-dev-tpod}) that hides the implementation
details from the developers and allow them to train a DNN object detector from a
web browser without writing a single line of code.

\subsection{Finite State Machine (FSM) as Application Representation}
\label{sec: app-dev-fsm-representation}

While the Gabriel platform handles data acquisition and transmission from mobile
wearable devices to a cloudlet, application developers still need to write
custom business logic for their own use cases. Since the task model can change
frequently during development, a fast turn-around time to implement changes can
reduce the overall development time. Therefore, we propose a finite state
machine representation of application logic and a GUI tool to enable fast
turn-around. 

Our choice of finite state machine representation is based on the observation
that WCAs, in their nature, consist of states. A FSM State corresponds to a
user state at a particular time instance. At a given state, the WCA runs
computer vision processing specific to the state to analyze the current scene.
We refer to the computer vision algorithms as Processors for the state. The
outputs of Processors are symbolic states of current scene. For example, a
symbolic state can be a vector of objects. Transitions to other states
happen when the symbolic states meet some criteria, for example, the presence of
a new object. We refer to these criteria as Predicates for transitions. A
State can have multiple transitions into different adjacent States. Each
Transition has its own Predicate. Transitions also have the concept
of precedence. The first Transition whose Predicate is satisfied is
triggered for a state change. In addition to predicates, Transitions also
have guidance instructions. When a Transition is taken, the corresponding
guidance instruction is delivered to the user.


\begin{figure}
  \centering
  \includegraphics[trim={0 0 0 0},width=\linewidth]{FIGS/fsm-example}
	\caption{Example WCA FSM}
    \label{figs:fsm-example}
\end{figure}

Figure~\ref{figs:fsm-example} shows an example of a simple WCA represented as a
finite state machine. This example WCA checks whether a piece of bread is
present. If so, it congratulates the user. On the other hand, if it detects a
piece of ham is present, a corrective guidance is sent to the user. There are
four states in total. The application starts from the ``Start'' state and
immediately transitions to the ``Check Bread'' state as the predicate is
``Always''. At the ``Check Bread'' state, for each frame, a DNN to detect bread
and ham is run to extract the symbolic state. Then, every transition is checked
to see if its predicate is satisfied. If a ``Bread'' is detected, the transition
predicate to the ``Finish'' state is satisfied and hence the transition taken.
The corresponding instruction ``Good Job'' is delivered to the user. Similarly,
when in ``Check Bread'' state, if a ``Ham'' is detected, the transition to the
``Error: Ham'' state is satisfied and taken. The instruction ``Replace Ham with
Bread'' would be delivered as the corrective guidance to the user.

With an FSM representing custom application logic, we impose structure on the
the WCA application. We provide a python API and a web GUI, introduced in
Section~\ref{sec: app-dev-fsm}, to enable developers to quickly create a
FSM-based cognitive assistant. 

\input{app-dev-tpod.tex}
\input{app-dev-fsm.tex}

\section{Chapter Summary and Discussion}

Wearable cognitive assistants are difficult to develop due to the high barrier of
entry and the lack of development methodology and tools. In this chapter, we
present a unifying development methodology, centered around object detection and
finite state machine representation to systematize the development process.
Based on this methodology, we build a suite of development tools that helps
object detector creation and speeds up task model implementation. 

% Previous
% experience with these tools, we have shown the productivity improvement can be 10x-20x for application
% developers.