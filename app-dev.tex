\chapter{Wearable Cognitive Assistance Development Tools}
\label{chapter: app-dev}

While previous chapters address the system challenges of scaling wearable
cognitive assistance at the edge, another key obstacle to the widespread
adoption of WCAs is the level of specialized skills and the long development
time needed to create new applications. Such expertise include task domain
knowledge, networked application development, and computer vision.
Researchers~\cite{chen2018application} report that it typically takes multiple
person-months of effort to create a single application. The majority of
development time is spent on learning new computer vision techniques, selecting
robust algorithms to use through trial and error, and implementing the
application. The high barrier of entry significantly limit the number of
wearable cognitive assistants available today. This is clearly not scalable. 

In this chapter, we reflect on existing ad hoc WCA development process, propose
a new development methodology centered around DNN-based object detection, and
introduce development tools to lower the barrier of WCA development. Our goal is
to simplify the process so that a small team (1-2 people) of a task expert and a
developer, without computer vision expertise, can create an initial version of a
Gabriel application within a few days. This is a productivity improvement of at
least one order of magnitude. Refinement and tuning may take longer, but can be
guided by early use of the application. 

Simplification is difficult. The application needs a precise definition of the
end point of each step (e.g., a particular screw mounted flush into a
workpiece), yet needs to be tolerant of alternative paths to reaching that end
point (e.g., hand-tightening versus using a screwdriver). We assume the small
development team has knowledge of the task specifics and general programming
skills, but not necessarily experiences with wearables and machine learning.

\section{Ad Hoc WCA Development Process}
\label{sec: app-dev-adhoc}

\begin{figure*}
  \centering
  \includegraphics[trim={0 6cm 0 0},width=\linewidth]{FIGS/ad-hoc-workflow}
	\caption{Ad Hoc Development Workflow}
    \label{fig:workflow}
\end{figure*}

Existing ad hoc development process of wearable cognitive assistance can be
described as figure~\ref{fig:workflow}. Developers first work with task expert
to identify and pinpoint a specific use case. With the use case in mind, the
development team need to identify essential visual states that can be reliably
recognized with machine vision. In the meantime, the use case is broken down
into individual steps. For each step, feedback to users are created based on
detected visual states. Potential errors are also enumerated and included as
additional steps. We refer to the complete set of the steps annotated with
visual states and feedback as the \textit{Task Model}. For example, for the LEGO
wearable cognitive assistance ~\cite{chen2017empirical}, the task models
contains the sequence of blocks to build up the final assembled lego piece,
together with potential misused blocks. The visual states to recognize are the
current pieces on the lego board, including the shape and the color of
individual lego block, and the composed shape.

Notably, a task model is not only determined by the task itself, but also
influenced heavily by visual states that can be reliably detected. In fact, it
is common to alter the sequence of steps or introduce additional steps to reduce
the difficulties and improve the reliability of computer vision recognition. In
fact, since there is a human in the loop, relying on humans to do what they are
good at is the main reason that wearable cognitive assistance can be implemented
without solving perception and planning problems intrinsic to robotics. For
example, a frequently used technique is to ask the user to hold the object of
interests at certain viewpoints as shown in the image guidance. Narrowing the
viewing angle makes the recognition problem more tractable.

The task model serves as a blueprint for implementation. For each step in the
task model, developers select and implement computer vision algorithms in order
to recognize the visual states. Custom logic is also written to handle step
transitions and interface with the Gabriel platform. After initial
implementation, test runs and measurements are conducted to evaluate the
robustness of computer vision checks and end-to-end application latency. The
implementation process is typically iterative to allow trials-and-errors in
choosing computer vision algorithms.

The ad hoc development process has unnecessary high requirements and burden for
WCA creators.
\begin{enumerate}
  \item They needs to be familiar with various computer vision algorithms to determine
what visual states can be recognized reliably. The knowledge of selecting
algorithms for specific scenarios often requires year of experience in
developing computer vision applications. This high bar of entry hinders more
WCA to be developed.
  \item It takes a significant amount of time to implement computer vision
algorithms for the trial-and-error workflow. Several months of coding time can
turn into a fruitless outcome when the algorithm is a bad fit. For WCAs with
tens or hundreds of visual states to be recognized, such manual implementation
is not scalable. Needed are automated methods to create visual state recognizers.
  \item The tight coupling of task model design and visual state recognition
calls for deep understanding in both task domain and computer vision. When
visual states in the task model end up being too difficult for the state-of-art
algorithms, the development team need to adjust the task model to either use
alternative methods or employing user assistance to identify the state. For
instance, in the RibLoc application, a task step is changed to asking the user
to read out a word on the gauge instead of performing optical character
recognition on the lightly-colored characters. Even worse, sometimes changes the
task workflow are needed. Therefore, close collaborations among task experts and
developers are needed due to this tight intertwinement between task model design
and computer vision algorithm exploration. Methodologies and tools that provides
quick turn-around when the task model changes can significantly reduce the
overall development time.
\end{enumerate}


% Among all the development procedures, creating the computer vision checks to
% detect user states consumes the most of development time and requires computer
% vision expertise and experience. With the adoption of DNNs, developers no longer
% need to spend days to select and tweak handcrafted features. Instead, the entire
% model is trained end-to-end using labeled data. However, DNNs, with millions of
% parameters to train, requires a significant amount of training data. Collecting
% and labeling data are time-consuming and painstaking. Besides, to craft and
% implement a DNN by hand is not trivial. Significant machine learning background
% is needed to tweak network architectures and parameters. Therefore, developer
% tools are needed to both help label the data and create deep neural networks.

% In summary, implementing the workflow of cognitive assistance takes time and
% efforts. Ad-hoc implementation requires a team of domain experts, developers and
% computer vision experts. Such development model cannot scale to thousands of
% applications. Therefore, Gabriel needs to be extended with tools to reduce the
% effort of creating wearable cognitive assistants.

% Existing ad-hoc approach to develop wearable cognitive assistance not only takes
% a long time, but also requires significant amount of computer vision expertise.
% Developers new to wearable cognitive assistance would need to spend months
% learning computer vision basics and acquire intuitions to determine what is
% achievable before developing an application. 

% Figure~\ref{fig:workflow} shows the ad-hoc development process. 

\section{A Quick Prototype Methodology}

To reduce the expertise and time in development, we propose a new prototype
methodology, centered around object detection for visual state detection and
finite state machine for application representation. We first introduce the
overview of this methodology in this section and then describe tools we have
built to empower this methodology in detail in subsequent sections.

To replace manual selection of computer vision algorithms through trial and
error, we propose to use DNN-based object detection as the fundamental building
blocks to detect user states. Each visual state should be decomposed and
described as individual objects within the scene along with the relationship
among these objects. We argue that using objects as fundamental building blocks
provide expressive and flexible constructions that are capable of describing a wide range of
user states. For example, it is trivial to detect the presence of figure 1.
(examples of expression). Moreover, for complicated (object close to each other
case). In addition, the relationship among objects can be not only spatial but
also temporal. For example, (use spatial as redundant information)

Existing applications can be easily fit into this object-centered methodology of
recognizing visual states. In Ping-Pong assistance, the Ping-Pong table, the
ball, and the opponent need to be recognized and localized. In Ikea Lamp
assistance, the lamp base, the shade, and the bulb need to be detected. (LEGO
case, shape and color can be detected as well)

The benefit of focusing on object detection as the central building block is an
automated method to develop visual detectors.  Object detection recently is
mature enough. (some words about the state of art in object detection). 
In
particular, recent advances in
DNNs~\cite{he2017mask},~\cite{Ren2015},~\cite{He2016} have not only drastically
improved the accuracy of object detection, but also provide an opportunity to
automate the creation of them. Unlike traditional CV algorithms, DNNs adopt a
end-to-end learning approach, in which features are no longer hand-crafted but
learned. The replacement of custom CV code with machine-learned models gives
automation opportunities. Nevertheless, creating a DNN-based object detector is
still both time-consuming and painstaking due to other reasons. DNNs have a lot
of parameters and requires millions of labeled examples to train from scratch.
Collecting and labeling these large amount of training data becomes a
bottleneck.

To reduce custom logic. Another is to use (state amchine representation to unify
the presentation of applications). These unified methodology provides methods
for automation. Examples applications represented in state machine format is
shown as below. We provide a state machine tool to help with this.

\input{app-dev-tpod.tex}
\input{app-dev-fsm.tex}
\section{Discussion}
