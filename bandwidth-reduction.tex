\chapter{Application-Agnostic Techniques to Reduce Network Transmission}
\label{chapter: bandwidth}

WCAs continuously stream sensor data to a cloudlet. The richer a sensing
modality is, the more information can be extracted. The core sensing modality of
WCAs is visual data, e.g. egocentric images and videos from wearable cameras.
Compared to other sensors, e.g. microphones and inertial measurement units
(IMUs), cameras capture visual information with rich semantics. As commercial
camera hardwares become more affordable, they are increasingly pervasive in
recent years. For example, in 2013, it is estimated that there are 5.9 million
security cameras in the UK~\cite{Barrett2013}. In the meantime, deep neural
networks (DNNs) have driven significant advancement in computer vision and have
achieved human-level accuracy in several previously untractable perception
problems (e.g. face recognition, image classification)~\cite{learned2016labeled,
schroff2015facenet}. The richness and the open-endness of visual data makes
camera the ideal sensor for WCAs.

However, continuous video transmission from many Tier-3 devices places severe
stress on the wireless spectrum.  Hulu estimates that its video streams require
13 Mbps for 4K resolution and 6 Mbps for HD resolution using highly optimized
offline encoding~\cite{Hulu2017}. Live streaming is less bandwidth-efficient, as
confirmed by our measured bandwidth of 10 Mbps for HD feed at 25 FPS. Just 50
users transmitting HD video streams continuously can saturate the theoretical
uplink capacity of 500 Mbps in a 4G LTE cell that covers a large rural
area~\cite{LteWorld2009}.  This is clearly not scalable.

In this chapter, we show how per-user bandwidth demand in WCA-like live video
analytics can be significantly reduced using an application-agnostic approach.
We aim to reduce bandwidth demand without compromising the timeliness or
accuracy of results. In contrast to previous
works~\cite{Wang2017networked,zhang2015design,Wang2016skyeyes}, we leverage
state-of-the-art deep neural networks (DNNs) to selectively transmit interesting
data from a video stream and explore environment-specific optimizations. 

This chapter is organized as follows. We first discuss the challenges of running
DNNs for visual perception solely on Tier-3 devices in
Section~\ref{bw:challenges}. Next, we propose and compare two
application-agnostic techniques to reduce network transmission. We present our
results first in the context of live video analytics for small autonomous
drones. Both as emerging Tier-3 devices, drones and wearable devices face
similar challenges in live video analysis. Finally, we showcase how these
techniques can be applied to WCAs in Section~\ref{bw:wca}.

\input{bandwidth-reduction-challenges}
\input{bandwidth-reduction-dumbdrone}
\input{bandwidth-reduction-earlydiscard}
\input{bandwidth-reduction-jitl}
\input{bandwidth-reduction-wca}

\section{Related Work}
\label{bw:relatedwork}

In the context of drone video analytics, Wang et al.~\cite{Wang2017networked}
shares our concern for wireless bandwidth, but focuses on coordinating a network
of drones to capture and broadcast live sport event. In addition, Wang et
al~\cite{Wang2016skyeyes} explored adaptive video streaming with drones using
content-based compression and video rate adaptation. While we share their
inspiration, our work leverages characteristics of DNNs to enable
mission-specific optimization strategies.

Much previous work on static camera networks explored efficient use of compute
and network resources at scale. Zhang et al.~\cite{zhang2017live} studied
resource-quality trade-off under result latency constraints in video analytics
systems. Kang et al.~\cite{kang2017noscope} worked on optimizing DNN queries
over videos at scale. While they focus on supporting a large number of computer
vision workload, our work optimizes for the first hop wireless bandwidth. In
addition, Zhang et al.~\cite{zhang2015design} designed a wireless distributed
surveillance system that supports a large geographical area through frame
selection and content-aware traffic scheduling. In contrast, our work does not
assume static cameras. We explore techniques that tolerate changing scenes in
video feeds and strategies that work for moving cameras.

Some previous work on computer vision in mobile settings has relevance to
aspects of our system design.  Chen et al.~\cite{chen2015glimpse} explore how
continuous real-time object recognition can be done on mobile devices. They meet
their design goals by combining expensive object detection with computationally
cheap object tracking.  Although we do not use object tracking in our work, we
share the resource concerns that motivate that work.  Naderiparizi et
al.~\cite{naderiparizi2017glimpse} describe a programmable early-discard camera
architecture for continuous mobile vision.  Our work shares their emphasis on
early discard, but differs in all other aspects.  In fact, our work can be
viewed as complementing that work: their programmable early-discard camera would
be an excellent choice for Tier-3 devices. Lastly, Hu et al~\cite{Hu2015} have
investigated the approach of using lightweight computation on a mobile device to
improve the overall bandwidth efficiency of a computer vision pipeline that
offloads computation to the edge.  We share their concern for wireless
bandwidth, and their use of early discard using inexpensive algorithms on the
mobile device.

\section{Chapter Summary and Discussion}
\label{bw:discussion}

In this chapter, we address the bandwidth challenge of running many WCAs at
scale. We propose two application-agnostic methods to reduce bandwidth
consumption when offloading computation to edge servers. 

The EarlyDiscard technique employs on-board filters to select interesting frames
and suppress the transmission of mundane frames to save bandwidth. In
particular, cheap yet effective DNN filters are trained offline to fully
leverage the large quantity of training data and the high learning capacities of
DNNs. Building on top of EarlyDiscard, JITL adapts an EarlyDiscard filter to a
specific environment online. While a WCA is running, JITL continuously evaluates
the EarlyDiscard filter and reduces the number of false positives by predicting
whether an EaryDiscard decision is made correctly. These two techniques together
reduce the total number of unnecessary frames transmitted. 

We evaluate these two strategies first in the drone live video analytics context
for search tasks in domains such as search-and-rescue, surveillance, and
wildlife conservation, and then for WCAs. Our experimental results show that
this judicious combination of Tier-3 processing and edge-based processing can
save substantial wireless bandwidth and thus improve scalability, without
compromising result accuracy or result latency. 
