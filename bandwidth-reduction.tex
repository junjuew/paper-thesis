\chapter{Application-Agnostic Techniques to Reduce Network Transmission}
\label{chapter: bandwidth}

WCAs continuously stream sensor data to the cloudlet. The richer a sensing
modality is, the more information can be extracted. The core sensing modality of
WCAs is visual data, e.g. egocentric images and videos from wearable cameras.
Compared to other sensors, e.g. microphones and inertial measurement units
(IMUs), cameras capture visual information with rich semantics. As commercial
camera hardware becomes more affordable, they become increasingly pervasive in
recent years. For example, in 2013, it is estimated that there are 5.9 million
security cameras in the UK~\cite{Barrett2013}. In the meantime, deep neural
networks (DNNs) have driven significant advancement in computer vision in recent
years and have achieved human-level accuracy in many previously untractable
perception problems (e.g. face recognition, image
classification)~\cite{learned2016labeled, schroff2015facenet}. The richness and
the open-endness of visual data makes camera the ideal sensor for WCAs.

However, continuous video transmission from many Tier-3 devices places severe
stress on the wireless spectrum.  Hulu estimates that its video streams require
13 Mbps for 4K resolution and 6 Mbps for HD resolution using highly optimized
offline encoding~\cite{Hulu2017}. Live streaming is less bandwidth-efficient, as
confirmed by our measured bandwidth of 10 Mbps for HD feed at 25 FPS. Just 50
users transmitting HD video streams continuously can saturate the theoretical
uplink capacity of 500 Mbps in a 4G LTE cell that covers a large rural
area~\cite{LteWorld2009}.  This is clearly not scalable.

In this chapter, we show how per-user bandwidth demand in WCA-like live video
analytics can be significantly reduced using an application-agnostic approach.
We aim to reduce bandwidth demand without compromising the timeliness or
accuracy of results. We present techniques for an adaptive computer vision
pipeline for Tier-3 devices that leverages edge computing to enable dynamic
optimizations. In contrast to previous
works~\cite{Wang2017networked}~\cite{zhang2015design}~\cite{Wang2016skyeyes}, we
leverage state-of-the-art deep neural networks (DNNs) to selectively transmit
interesting data from a video stream and explore environment-specific
optimizations. 

This chapter is organized as follows. We first discuss the challenges of running
DNNs for visual perception solely on Tier-3 devices in
Section~\ref{bw:challenges}. Next, we propose and compare two
application-agnostic techniques to reduce network transmission in edge computing
context. We present our results first in the context of live video analytics for
small autonomous drones. Both as emerging Tier-3 devices, drones and wearable
devices face similar challenges in live video analysis. Finally, we showcase how
these techniques can be applied to WCAs in Section~\ref{bw:wca}.

\input{bandwidth-reduction-challenges}
\input{bandwidth-reduction-dumbdrone}
\input{bandwidth-reduction-earlydiscard}
\input{bandwidth-reduction-jitl}
\input{bandwidth-reduction-wca}

\section{Discussion}
\label{bw:discussion}

The EarlyDiscard technique employs on-board filters to select interesting
frames and suppress the transmission of mundane frames to save bandwidth. In
particular, cheap yet effective DNN filters are trained offline to fully
leverage the large quantity of training data and the high learning capacities of
DNNs. Building on top of EarlyDiscard, JITL adapts an EarlyDiscard filter to a
specific mission environment online. Throughout a flight, JITL continuously
evaluates the EarlyDiscard filter and reduces the number of false positives by
predicting whether an EaryDiscard decision is made correctly. These two
techniques together reduce the total number of unnecessary frames transmitted.
In addition, some missions need consecutive frames instead of individual images
to do tasks such as activity recognition. Reachback compensates for these
scenarios when EarlyDiscard and JITL are deployed. Once the cloudlet identifies
an interesting frame from the data sent back by the drone, nearby frames are
pulled from storage on the drone. Furthermore, either an algorithm or a person
in the loop can determine when to trigger reachback. Besides reachback, the
person in the loop may also identify unique characteristics to create more
effective context-aware filters to increase accuracy and reduce on-board
computation.

\section{Related Work}
\label{bw:relatedwork}

The work presented in this paper is disjoint from these previous drone-centric
research efforts. Our focus is on reducing wireless transmission for live video
from autonomous drones in use cases such as search and rescue, surveillance, and
wildlife conservation. Wang et al.~\cite{Wang2017networked} shares our concern
for wireless bandwidth, but focuses on coordinating a network of drones to
capture and broadcast live sport event. In addition, Wang et
al~\cite{Wang2016skyeyes} explored adaptive video streaming with drones using
content-based compression and video rate adaptation. While we share their
inspiration, our work leverages characteristics of DNNs and explore human-in-the
loop to enable mission-specific optimization strategies including reachback and
context-awareness.

Much previous work on static camera networks and video analytics systems
explored efficient use of compute and network resources at scale. Zhang et
al.~\cite{zhang2017live} studied resource-quality trade-off under result latency
constraints in video analytics systems. Kang et al.~\cite{kang2017noscope} worked
on optimizing DNN queries over videos at scale. While they focus on supporting a
large number of computer vision workload, our work optimizes for the first hop
wireless bandwidth. In addition, Zhang et al.~\cite{zhang2015design} designed a
wireless distributed surveillance system that supports a large geographical area
through frame selection and content-aware traffic scheduling. In contrast, our
work uses drone moving cameras. We explore techniques that tolerate changing
scenes in video feeds and strategies that can leverage the human operator.

Some previous work on computer vision in mobile settings has relevance to
aspects of our system design.  Chen et al.~\cite{chen2015glimpse} explore how
continuous real-time object recognition can be done on mobile devices. They meet
their design goals by combining expensive object detection with computationally
cheap object tracking.  Although we do not use object tracking in our work, we
share the resource concerns that motivate that work.  Naderiparizi et
al.~\cite{naderiparizi2017glimpse} describe a programmable early-discard camera
architecture for continuous mobile vision.  Our work shares their emphasis on
early discard, but differs in all other aspects.  In fact, our work can be
viewed as complementing that work: their programmable early-discard camera would
be an excellent choice for our drones. Lastly, Hu et al~\cite{Hu2015} have
investigated the approach of using lightweight computation on a mobile device to
improve the overall bandwidth efficiency of a computer vision pipeline that
offloads computation to the edge.  We share their concern for wireless
bandwidth, and their use of early discard using inexpensive algorithms on the
mobile device.  However, their work is not in a drone setting and has no
counterpart to just-in-time learning, reachback, or context-aware discard
described in our work.
