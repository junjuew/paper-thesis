\chapter{Introduction}
\label{chapter: intro}

It has been a long endeavour to augment human cognition with machine
intelligence. As early as in 1945, Vannevar Bush envisioned a machine Memex that
provides "enlarged intimate supplement to one's memory" and can be "consulted
with exceeding speed and flexibility" in the seminal article \textit{As We May
Think}~\cite{bush1945we}. This vision has been brought closer to reality by
years of research in computing hardware, artificial intelligence, and
human-computer interaction. In late 90s to early 2000s, Smailagic et al.
~\cite{smailagic1993case,smailagic1998very,smailagic2002application} created
prototypes of wearable computers to assist cognitive tasks. For example, they
displayed inspection manuals in a head-up screen to facilitate aircraft
maintenance. Around the same time, Loomis et
al.~\cite{loomis1998navigation,loomis1994personal} explored using computers
carried in a backpack to provide auditory cues in order to help the blind
navigate.  Davis et al.~\cite{davies1998developing,cheverst2000developing}
developed a context-sensitive intelligent visitor guide leveraging hand-portable
multimedia systems. While these research works pioneered cognitive assistance
and its related fields, their robustness and functionality were limited by the
supporting technologies of their time.

More recently, as the underlying technologies experience significant
advancement, a new genre of applications, Wearable Cognitive Assistance
(WCA)~\cite{ha2014towards,chen2018application}, has emerged that pushes
the boundaries of augmented cognition. WCA applications continuously process
data from body-worn sensors and provide just-in-time guidance to help a user
complete a specific task. For example, an IKEA Lamp
assistant~\cite{chen2018application} has been built to assist the assembly of a
table lamp. To use the application, a user wears a head-mounted smart glass that
continuously captures her actions and surroundings from a first-person
viewpoint. In real-time, the camera stream is analyzed to identify the state of
the assembly. Audiovisual instructions are generated based on the detected
state. The instructions either demonstrate a subsequent procedure or alert and
correct a mistake.

Since its conceptualization in 2004~\cite{Satya2004}, WCA has attracted much
research interest from both academia and industry. The building blocks for its
vision came into place by 2014, enabling the first implementation of this
concept in {\em Gabriel}~\cite{ha2014towards}. In 2017, Chen et
al~\cite{chen2017empirical} described a number of applications of this genre,
quantified their latency requirements, and profiled the end-to-end latencies of
their implementations. In late 2017, SEMATECH and DARPA jointly funded \$27.5
million of research on such applications~\cite{Oakley2018, Stokes2018}.  At the
Mobile World Congress in February 2018, wearable cognitive assistance was the
focus of an entire session~\cite{Ray2018}.  For AI-based military use cases,
this class of applications is the centerpiece of ``Battlefield
2.0''~\cite{Doffman2018}. By 2019, WCA was being viewed as a prime source of
``killer apps'' for edge computing~\cite{Satya2019b,Satya2019c}.


Different from previous research efforts, the design goals of WCA advance the
frontier of mobile computing in multiple aspects. First, wearable devices,
particularly head-mounted smart glasses, are used to reduce the discomfort
caused by carrying a bulky computation device. Users are freed from holding a
smartphone and therefore able to interact with the physical world using both
hands. The convenience of this interaction model comes at the cost of
constrained computation resources. The small form-factor of smart glasses
significantly limits their onboard computation capability due to size, cooling,
and battery life reasons. Second, placed at the center of computation is the
unstructured high-dimensional image and video data. Only these data types can
satisfy the need to extract rich semantic information to identify the progress
and mistakes a user makes. Furthermore, state-of-art computer vision algorithms
used to analyze image data are both compute-intensive and challenging to
develop. Third, many cognitive assistants give real-time feedback to users and
have stringent end-to-end latency requirements. An instruction that arrives too
late often provides no value and may even confuse or annoy users. This
latency-sensitivity further increases their high demands of system resource and
optimizations.

To meet the latency and the compute requirements, previous research leverages
edge computing and offloads computation to a cloudlet. A
cloudlet~\cite{satyanarayanan2009case} is a small data-center located at the
edge of the Internet, one wireless hop away from users. Researchers have
developed an application framework for wearable cognitive assistance, named
Gabriel, that leverages cloudlets, optimizes for end-to-end latency, and eases
application
development~\cite{chen2018application,ha2014towards,chen2017empirical}. On top
of Gabriel, several prototype applications have been built, such as PINGPONG
Assistance, LEGO Assistance, COOKING Assistance, and IKEA LAMP Assembly
Assistance. Using these applications as benchmarks, Chen et
al.~\cite{chen2017empirical} presented empirical measurements detailing the
latency contributions of individual system components. Furthermore, a
multi-algorithm approach was proposed to reduce the latency of computer vision
computation by executing multiple algorithms in parallel and conditionally
selecting a fast and accurate algorithm for the near future.

While previous research has demonstrated the technical feasibility of wearable
cognitive assistants and meeting latency requirements, many practical concerns
have not been addressed. First, previous work operates the wireless networks and
cloudlets at low utilization in order to meet application latency. The economics
of practical deployment precludes operation at such low utilization. In
contrast, resources are often highly utilized and congested when serving many
users. How to efficiently scale Gabriel applications to a large number of users
remains to be answered. Second, previous work on the Gabriel framework reduces
application development efforts by managing client-server communication, network
flow control, and cognitive engine discovery. However, the framework does not
address the most time-consuming parts of creating a wearable cognitive
assistance application. Experience has shown that developing computer vision
modules that analyze video feeds is a time-consuming and painstaking process
that requires special expertise and involves rounds of trials and errors.
Development tools that alleviate the time and the expertise needed can greatly
facilitate the creation of these applications.


\section{Thesis Statement}

In this dissertation, we address the problem of \textit{scaling wearable
cognitive assistance}. Scalability here has a two-fold meaning. First, a
scalable system supports a large number of associated clients with fixed amount
of infrastructure, and is able to serve more clients as resources increase.
Second, we want to enable a small software team to quickly create, deploy, and
manage these applications. We claim that:

\textbf{Two critical challenges to the widespread adoption of wearable cognitive
  assistance are 1) the need to operate cloudlets and wireless network at low
  utilization to achieve acceptable end-to-end latency 2) the level of specialized
  skills and the long development time needed to create new applications. These
  challenges can be effectively addressed through system optimizations,
  functional extensions, and the addition of new software development tools to
  the Gabriel platform.}


We validate this thesis in this dissertation. The main contributions of the
dissertation are as follows:
\begin{enumerate}
  \item{We propose application-agnostic and application-aware techniques to
  reduce bandwidth consumption and offered load when the cloudlet is
  oversubscribed.}
  \item{We provide a profiling-based cloudlet resource allocation mechanism that
  takes account of diverse application adaptation characteristics.}
  \item{We propose a new prototyping methodology and create a suite of
  development tools to reduce the time and lower the barrier of entry for WCA
  creation.}
\end{enumerate}

\section{Thesis Overview}

The remainder of this dissertation is organized as follows.

\begin{itemize}
  \item{In Chapter~\ref{chapter: background}, we introduce prior work in wearable cognitive assistance.}
  \item{In Chapter~\ref{chapter: bandwidth}, we describe and evaluate application-agnostic techniques to reduce bandwidth consumption when offloading computation.}
  \item{In Chapter~\ref{chapter: load}, we propose and evaluate 
  application-specific techniques to reduce offered load. 
  We demonstrate their effectiveness 
  with minimal impact on result latency.}
  \item{In Chapter~\ref{chapter: cloudlet}, we present a resource management
  mechanisms that takes application adaptation characteristics into account to
  optimize system-wide metrics.}
  \item{In Chapter~\ref{chapter: app-dev}, we introduce a methodology and
  development tools for quick prototyping.}
  \item{In Chapter~\ref{chapter: conclusion}, we conclude this dissertation and discuss future directions.}
\end{itemize}