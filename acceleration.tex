\section{Gabriel Acceleration Framework}

DNNs have been widely adopted as the de facto algorithms to use for analyzing
the image and video data. Therefore, running DNNs to analyze images is a common, if
not the most common, workload Gabriel needs to support. Due to the parallel
nature of DNN computation, such workload can be accelerated by special
hardware, including GPUs, FPGAs, and custom ASICs.

On cloudlets, due to physical constraints and economic decisions, we are
likely to see a wide range of heterogeneous accelerators. Some edge deployment
may have a cluster of custom ASIC DNN accelerators with proprietary drivers.
Others might use commodity GPUs. Some may not have any accelerators at all. Such
heterogeneity of acceleration capabilities has a direct impact on application
latencies and the number of users an edge node can serve. While hardware
capabilities vary, applications' needs on acceleration resources vary as well.
Applications may use DNNs of different sizes based on the complexity of their
task. For example, the Face Recognition Assistant developed uses a much smaller
network than the Sandwich Assistant in~\cite{chen2017empirical}. As a result,
the Face Assistant does not need a GPU to meet its latency requirement while
the Sandwich Assistant does.

Given the heterogeneity in both hardware capabilities and application demands, I
plan to study how to effectively federate cloudlets with different accelerators
to better serve applications with different needs. The heterogeneity can be both
inter-cloudlet and intra-cloudlet. I plan to address inter-cloudlet
heterogeneity from a cloudlet discovery perspective. Specifically, I want to focus
on how to maximize the number of users served by carefully discovering and
selecting appropriate cloudlets to offload at run-time based on the availability
of accelerators, their utilization, and the potentials to meet latency
requirements. For instance, different applications from the same mobile client
may be served by different cloudlets to accommodate their needs for accelerators and
the requirements of latency.

In addition, within a cloudlet, there can be machines with and without
accelerators. Virtualization for accelerators is difficult to develop and often
cannot be easily adapted to new hardware. Instead, I plan to study how to enable
the sharing of accelerators within a cloudlet on the application layer. One
approach I have implemented is to expose the accelerator as an HTTP endpoint of
fixed functions. This method is similar to model serving systems described
in~\cite{olston2017tensorflow}~\cite{crankshaw2017clipper}. Such approach faces
a trade-off between throughput and latency. Specifically, the more frames a
server batches, the higher throughput the server can achieve. I plan to explore
such trade-off in wearable cognitive assistance context.
